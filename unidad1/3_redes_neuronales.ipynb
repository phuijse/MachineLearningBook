{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import animation\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material teórico\n",
    "\n",
    "[Slides](https://docs.google.com/presentation/d/1IJ2n8X4w8pvzNLmpJB-ms6-GDHWthfsJTFuyUqHfXg8/edit?usp=sharing)"
   ]
  },
  {
   "attachments": {
    "tensor_illustration.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breve tutorial de [PyTorch](https://pytorch.org) \n",
    "\n",
    "PyTorch es una librería de alto nivel para Python que provee \n",
    "1. Una clase tensor para hacer cómputo de alto rendimiento \n",
    "1. Un plataforma para crear y entrenar redes neuronales\n",
    "\n",
    "### Torch Tensor\n",
    "\n",
    "La clase [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) es muy similar en uso al `ndarray` de [*NumPy*](https://numpy.org/)\n",
    "\n",
    "Un tensor corresponde a una matriz o arreglo n-dimensional con tipo definido que soporta operaciónes vectoriales tipo SIMD y broadcasting\n",
    "\n",
    "\n",
    "![tensor_illustration.png](attachment:tensor_illustration.png)\n",
    "\n",
    "La documentación de la clase con todas las operaciones que soporta: https://pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "A continuación revisaremos las más fundamentales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de tensores\n",
    "\n",
    "Un tensor puede crearse usando constructores de torch o a partir de datos existentes: lista de Python o *ndarray* de NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un tensor de 10 ceros\n",
    "display(torch.zeros(10))\n",
    "# Un tensor de 10 unos\n",
    "display(torch.ones(10))\n",
    "# Un tensor de números linealmente espaciados\n",
    "display(torch.linspace(0, 9, steps=10))\n",
    "# Un tensor de 10 números aleatorios con distribución N(0, 1)\n",
    "display(torch.randn(10))\n",
    "# Un tensor creado a partir de una lista\n",
    "display(torch.Tensor([0, 1, 2, 3, 4, 5, 6]))\n",
    "# Un tensor creado a partir de un ndarray\n",
    "numpy_array = np.random.randn(10)\n",
    "display(torch.from_numpy(numpy_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atributos importantes de los tensores\n",
    "\n",
    "Un tensor tiene un tamaño (dimesiones) y tipo específico\n",
    "\n",
    "Un tensor puede estar alojado en la memoria del sistema ('cpu') o en la memoria de dispositivo ('gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(10, 20, 30)\n",
    "display(a.shape)\n",
    "display(a.dtype)\n",
    "display(a.device)\n",
    "display(a.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se crea un tensor se puede especificar el tipo y el dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(10, dtype=torch.int32, device='cuda')\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulación de tensores\n",
    "\n",
    "Podemos manipular la forma de un tensor usando: reshape, flatten, roll, traspose, unsqueeze, entre otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(0, 9, 10)\n",
    "display(a)\n",
    "display(a.reshape(2, 5))\n",
    "display(a.reshape(2, 5).t())\n",
    "display(a.reshape(2, 5).flatten())\n",
    "display(a.roll(2))\n",
    "display(a.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculos con tensores\n",
    "\n",
    "Un tensor soporta operaciones aritméticas y lógicas\n",
    "\n",
    "Si el tensor está en memoria de sistema entonces las operaciones son realizadas por la CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(0, 5, steps=6)\n",
    "display(a)\n",
    "# aritmética y funciones\n",
    "display(a + 5)\n",
    "display(2*a)\n",
    "display(a.pow(2))\n",
    "display(a.log())\n",
    "# máscaras booleanas\n",
    "display(a[a>3])\n",
    "# Operaciones con otros tensores\n",
    "b = torch.ones(6)\n",
    "display(a + b)\n",
    "display(a*b)\n",
    "# broadcasting\n",
    "display(a.unsqueeze(1)*b.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculos en GPU\n",
    "\n",
    "Usando el atributo `to` podemos intercambiar un tensor entre GPU ('device') y CPU ('host')\n",
    "\n",
    "Cuando todos los tensores involucrados en una operaciones están en memoria de dispositivo entonces el cálculo lo hace la GPU\n",
    "\n",
    "La siguiente nota indica las opciones para intercambiar datos entre GPU y CPU que ofrece PyTorch: https://pytorch.org/docs/stable/notes/cuda.html \n",
    "\n",
    "##### Breve nota: \n",
    "Una *Graphical Processing Unit* (GPU) o tarjeta de video es un hardware para hacer cálculos sobre mallas tridimensionales, generación de imágenes (rendering) y otras tareas gráficas. A diferencia de la CPU, la GPU es especialista en cálculo paralelo y tiene miles de nucleos (NVIDIA RTX 2080: 2944 nucleos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(10)\n",
    "display(a.device)\n",
    "a = a.to('cuda')\n",
    "display(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-diferenciación\n",
    "\n",
    "Las redes neuronales se entrenan usando **Gradiente descedente**\n",
    "\n",
    "> Necesitamos calcular las derivadas de la función de costo para todos los parámetros de la red\n",
    "\n",
    "Esto puede ser complejo si nuestra red es grande y tiene distintos tipos de capas\n",
    "\n",
    "PyTorch viene incorporado con un sistema de diferenciación automática denominado [`autograd`](https://pytorch.org/docs/stable/autograd.html) \n",
    "\n",
    "Para poder derivar una función en pytorch\n",
    "\n",
    "1. Se necesita que su entrada sean tensores con el atributo `requires_grad=True`\n",
    "1. Luego llamamos la función `backward()` de la función\n",
    "1. El resultado queda guardado en el atributo `grad` de la entrada (nodo hoja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 10, steps=1000, requires_grad=True)\n",
    "\n",
    "y = 5*x -20\n",
    "#y = torch.sin(2.0*np.pi*x)\n",
    "#y = torch.sin(2.0*np.pi*x)*torch.exp(-(x-5).pow(2)/3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(x.detach().numpy(), y.detach().numpy(), label='y')\n",
    "\n",
    "y.backward(torch.ones_like(x))\n",
    "\n",
    "ax.plot(x.detach().numpy(), x.grad.detach().numpy(), label='dy/dx')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafo de cómputo\n",
    "\n",
    "Cuando contatenamos operaciones PyTorch construye internamente un \"grafo de cómputo\"\n",
    "\n",
    "$$\n",
    "x \\to z = f_1(x) \\to y = f_2(z)\n",
    "$$\n",
    "\n",
    "La función `backward` calcula los gradientes y los almacena en los nodo hoja que tengan `requires_grad=True`\n",
    "\n",
    "Por ejemplo\n",
    "\n",
    "    y.backward : Guarda dy/dx en x.grad\n",
    "    \n",
    "    z.backward : Guarda dz/dx en x.grad\n",
    "\n",
    "Basicamente `backward` implementa la regla de la cadena de las derivadas\n",
    "\n",
    "`backward` recibe una entrada: La derivada de la etapa superior de la cadena. Por defecto usa `torch.ones([1])`, asume que se está en el nivel superior y que la salida es escalar (unidimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 10, steps=1000, requires_grad=True) # Nodo hoja\n",
    "display(x.grad_fn)\n",
    "z = torch.sin(x)\n",
    "display(z.grad_fn)\n",
    "y = z.pow(2)\n",
    "display(y.grad_fn)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(x.detach().numpy(), z.detach().numpy(), label='z')\n",
    "ax.plot(x.detach().numpy(), y.detach().numpy(), label='y')\n",
    "# Derivada dy/dx\n",
    "y.backward(torch.ones_like(x), create_graph=True)\n",
    "ax.plot(x.detach().numpy(), x.grad.detach().numpy(), label='dy/dx')\n",
    "# Borro el resultado en x.grad\n",
    "x.grad = None\n",
    "# Derivada dz/dx\n",
    "z.backward(torch.ones_like(x))\n",
    "ax.plot(x.detach().numpy(), x.grad.detach().numpy(), label='dz/dx')\n",
    "plt.legend();\n",
    "#ax.plot(x.detach().numpy(), 2*np.cos(x.detach().numpy())*np.sin(x.detach().numpy()), label='dy/dx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Redes Neuronales Artificiales en PyTorch\n",
    "\n",
    "PyTorch nos ofrece la clase tensor y las funcionalidades de autograd\n",
    "\n",
    "Estas poderosas herramientas nos dan todo lo necesario para construir y entrenar redes neuronales artificiales\n",
    "\n",
    "Para facilitar aun más estas tareas PyTorch tiene módulos de alto nivel que implementan\n",
    "\n",
    "1. Modelo base de red neuronal: `torch.nn.Module`\n",
    "1. Distintos tipos de capas, funciones de activación y funciones de costo: [`torch.nn`](https://pytorch.org/docs/stable/nn.html)\n",
    "1. Distintos algoritmos de optimización basados en gradiente descedente: [`torch.optim`](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "\n",
    "Una red neuronal en PyTorch se implementa\n",
    "1. Heredando de [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#module)\n",
    "1. Especificando las funciones `__init__` y `forward`\n",
    "\n",
    "Otra opción es usar [`torch.nn.Sequential`](https://pytorch.org/docs/stable/nn.html#sequential) y especificar una lista de capas\n",
    "\n",
    "\n",
    "#### Red MLP en pytorch:\n",
    "\n",
    "Heredamos de `Module` y especificamos el constructor y la función `forward`\n",
    "\n",
    "Creamos una red de dos entradas, una capa oculta y una neurona salida\n",
    "\n",
    "La capa `torch.nn.Linear` con parámetro $W$ y $b$ realiza la siguiente operación sobre la entrada $X$\n",
    "\n",
    "$$\n",
    "Z = WX + b\n",
    "$$\n",
    "\n",
    "corresponde a la capa completamente conectada (*fully-connected*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    # Constructor: Crea las capas de la red\n",
    "    def __init__(self, input_dim=2, hidden_dim=10, output_dim=1): \n",
    "        # Mandatorio: Llamar al constructor del padre:\n",
    "        super(MLP, self).__init__()  \n",
    "        # Creamos dos capas completamente conectadas\n",
    "        self.hidden = torch.nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.output = torch.nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "        # Función de activación sigmoide\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "        \n",
    "    # Forward: Conecta la entrada con la salida\n",
    "    def forward(self, x):\n",
    "        # Pasamos x por la primera capa y luego aplicamos función de activación\n",
    "        z = self.activation(self.hidden(x))\n",
    "        # Pasamos el resultado por la segunda capa y lo retornamos\n",
    "        return self.activation(self.output(z))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al crear una capa `Linear` de forma interna se registran los parámetros `weight` y `bias`, ambos con `requires_grad=True`\n",
    "\n",
    "Inicialmente los parámetros tienen valores aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(hidden_dim=2)\n",
    "display(model.hidden.weight)\n",
    "display(model.hidden.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos evaluar el modelo sobre un tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 10*torch.rand(10000, 2) - 5\n",
    "Y = model.forward(X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "X_numpy = X.detach().numpy()\n",
    "Y_numpy = Y.detach().numpy()\n",
    "ax.scatter(X_numpy[:, 0], X_numpy[:, 1], s=10, c=Y_numpy[:, 0], cmap=plt.cm.RdBu_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Para entrenar la neurona debemos definir \n",
    "\n",
    "- Una función de costo\n",
    "- Un algoritmo de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de costo entropía cruzada binaria\n",
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "# Algoritmo de optimización Gradiente Descendente Estocástico\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digamos que tenemos un dato $X$ y una etiqueta $Y$\n",
    "\n",
    "Podemos calcular el error de clasificar ese dato con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[-1.0, 1.0]])\n",
    "Y = torch.tensor([[0.]])\n",
    "\n",
    "hatY = model.forward(X)\n",
    "display(hatY)\n",
    "loss = criterion(hatY, Y)\n",
    "display(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que calculamos la loss podemos calcular el gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "display(model.output.weight.grad)\n",
    "display(model.output.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente actualizamos los parámetros usando la función `step` de nuestro optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetea los gradientes\n",
    "display(model.output.weight)\n",
    "display(model.output.bias)\n",
    "optimizer.step()\n",
    "display(model.output.weight)\n",
    "display(model.output.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos este proceso a través de varias \"épocas\" de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nepoch in range(10):\n",
    "    # Calculamos la salida del modelo\n",
    "    hatY = model.forward(X)\n",
    "    # Reseteamos los gradientes de la iteración anterior\n",
    "    optimizer.zero_grad()\n",
    "    # Calculamos la función de costo\n",
    "    loss = criterion(hatY, Y)\n",
    "    # Calculamos su gradiente\n",
    "    loss.backward()\n",
    "    # Actualizamos los parámetros\n",
    "    optimizer.step()\n",
    "    print(\"%d w:%f %f b:%f\" %(nepoch, model.output.weight[0, 0], model.output.weight[0, 1], model.output.bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenando en un conjunto de datos\n",
    "\n",
    "Consideremos un conjunto de entrenamiento con datos bidimensionales y dos clases como el siguiente\n",
    "\n",
    "Notemos que no es linealmente separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "data, labels = sklearn.datasets.make_circles(n_samples=1000, noise=0.2, factor=0.25)\n",
    "#data, labels = sklearn.datasets.make_moons(n_samples=1000, noise=0.2)\n",
    "#data, labels = sklearn.datasets.make_blobs(n_samples=[250]*4, n_features=2, cluster_std=0.5,\n",
    "#                                          centers=np.array([[-1, 1], [1, 1], [-1, -1], [1, -1]]))\n",
    "#labels[labels==2] = 1; labels[labels==3] = 0;\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "for k, marker in enumerate(['x', 'o']):\n",
    "    ax.scatter(data[labels==k, 0], data[labels==k, 1], s=20, marker=marker, alpha=0.75)\n",
    "    \n",
    "# Para las gráficas\n",
    "x_min, x_max = data[:, 0].min() - 0.5, data[:, 0].max() + 0.5\n",
    "y_min, y_max = data[:, 1].min() - 0.5, data[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Antes de empezar el entrenamiento convertimos los datos a formato tensor de PyTorch\n",
    "1. Luego presentamos los datos en *mini-batches* a la red neuronal en cada época del entrenamiento\n",
    "\n",
    "PyTorch provee las clases `DataSet` y `DataLoader` para lograr estos objetivos\n",
    "\n",
    "Estas clases son parte del módulo data: https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "En este caso crearemos un set a partir de tensores usando una clase que hereda de `DataSet`\n",
    "\n",
    "    TensorDataset(*tensors)\n",
    "    \n",
    "Luego crearemos conjuntos de entrenamiento y validación usando \n",
    "\n",
    "    Subset(dataset, indices)\n",
    "    \n",
    "Finalmente crearemos dataloaders usando\n",
    "\n",
    "    DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "# Separamos el data set en entrenamiento y validación\n",
    "train_idx, valid_idx = next(sklearn.model_selection.ShuffleSplit(train_size=0.6).split(data, labels))\n",
    "\n",
    "\n",
    "# Crear conjuntos de entrenamiento y prueba\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset \n",
    "\n",
    "# Creamos un conjunto de datos en formato tensor\n",
    "torch_set = TensorDataset(torch.from_numpy(data.astype('float32')), \n",
    "                          torch.from_numpy(labels.astype('float32')))\n",
    "\n",
    "# Data loader de entrenamiento\n",
    "torch_train_loader = DataLoader(Subset(torch_set, train_idx), shuffle=True, batch_size=32)\n",
    "# Data loader de validación\n",
    "torch_valid_loader = DataLoader(Subset(torch_set, valid_idx), shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los `DataLoader` se ocupan como iteradores de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_data, sample_label in torch_train_loader:\n",
    "    display(sample_data.shape)\n",
    "    display(sample_label.shape)\n",
    "    display(sample_label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recordemos\n",
    "\n",
    "Para cada dato de entrenamiento:\n",
    "- Calculamos gradientes: `loss.backward`\n",
    "- Actualizamos parámetros `optimizer.step`\n",
    "\n",
    "Para cada dato de validación\n",
    "- Evaluamos la *loss* para detectar sobre-ajuste\n",
    "\n",
    "> Una pasada por todos los datos se llama: época\n",
    "\n",
    "#### ¿Cuándo nos detenemos?\n",
    "\n",
    "Lo ideal es detener el entrenamiento cuando la loss de validación no haya disminuido durante una cierta cantidad de épocas\n",
    "\n",
    "Podemos usar [`save`](https://pytorch.org/tutorials/beginner/saving_loading_models.html) para ir guardando los parámetros del mejor modelo de validación\n",
    "\n",
    "Usamos un número fijo de épocas como resguardo: Si el modelo no ha convergido entonces debemos incrementarlo\n",
    "\n",
    "#### ¿Cómo afecta el resultado el número de neuronas en la capa oculta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(hidden_dim=3)\n",
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "n_epochs = 200\n",
    "running_loss = np.zeros(shape=(n_epochs, 2))\n",
    "\n",
    "best_valid = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(k, model, criterion, optimizer):\n",
    "    global best_valid\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "    \n",
    "    # Loop de entrenamiento\n",
    "    for sample_data, sample_label in torch_train_loader:\n",
    "        output = model.forward(sample_data)\n",
    "        optimizer.zero_grad()        \n",
    "        loss = criterion(output, sample_label.unsqueeze(1))  \n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Loop de validación\n",
    "    for sample_data, sample_label in torch_valid_loader:\n",
    "        output = model.forward(sample_data)\n",
    "        loss = criterion(output, sample_label.unsqueeze(1))  \n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "    # Guardar modelo si es el mejor hasta ahora\n",
    "    if k % 10 == 0:\n",
    "        if valid_loss < best_valid:\n",
    "            best_valid = valid_loss\n",
    "            torch.save({'epoca': k,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': valid_loss}, '/home/phuijse/modelos/best_model.pt')\n",
    "    \n",
    "    return train_loss/torch_train_loader.dataset.__len__(), valid_loss/torch_valid_loader.dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot(k):\n",
    "    global model, running_loss\n",
    "    [ax_.cla() for ax_ in ax]\n",
    "    running_loss[k, 0], running_loss[k, 1] = train_one_epoch(k, model, criterion, optimizer)\n",
    "    Z = model.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "    Z = Z.detach().numpy().reshape(xx.shape)\n",
    "    ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=1., vmin=0, vmax=1)\n",
    "    for i, (marker, name) in enumerate(zip(['o', 'x'], ['Train', 'Test'])):\n",
    "        ax[0].scatter(data[labels==i, 0], data[labels==i, 1], color='k', s=10, marker=marker, alpha=0.5)\n",
    "        ax[1].plot(np.arange(0, k+1, step=1), running_loss[:k+1, i], '-', label=name+\" cost\")\n",
    "    plt.legend(); ax[1].grid()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3.5), tight_layout=True)\n",
    "update_plot(0)\n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=n_epochs, \n",
    "                               interval=10, repeat=False, blit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuronas de capa oculta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, model.hidden.out_features, figsize=(8, 3), tight_layout=True)\n",
    "\n",
    "Z = model.hidden(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32'))).detach().numpy()\n",
    "Z = 1/(1+np.exp(-Z))\n",
    "for i in range(model.hidden.out_features):\n",
    "    ax[i].contourf(xx, yy, Z[:, i].reshape(xx.shape), \n",
    "                   cmap=plt.cm.RdBu_r, alpha=1., vmin=np.amin(Z), vmax=np.amax(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recuperando el mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(hidden_dim=3)\n",
    "\n",
    "print(\"state_dict del módelo:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor])\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(torch.load('/home/phuijse/modelos/best_model.pt')['model_state_dict'])\n",
    "\n",
    "print(\" \")\n",
    "print(\"state_dict del módelo recuperado:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
